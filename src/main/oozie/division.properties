##########
# CLUSTER
releaseLabel=01.01.01
nameNode=hdfs://nameservice1
jobTracker=yarnRM
# Facility: derived parameters based on custom values
jobName=${sourceName}-stage-0-${fourDigitId}
tableDefFile=athena_table_defs.csv
################## START CUSTOM ################
################## DATABASE CONNECTIONS##########
# Teradata DATABASE FOR LANDING ZONE CREATION
tdStageDB=EDW_STAGE_V2
# The Teradata databaase to which we will write status records
tdAuditDB=EDW_AUDIT_CNTL_VIEWS
# The netscaler address of the instance we are writing status records to
tdServer=XXXX
# The user identity needed to write status records to the teradata instance
tdUserID=Corp_INFAETL_svc
# The user password needed to write status records to the teradata instance
tdUserIDPassword=XXXX
################## DATABASE CONNECTIONS##########
# sourceSystemName lower-case: athena, dar, meditech ...
sourceName=athena
# The amount of time in minute between when the coordinator attempts to run
coordTimeInterval=60
################# JavaMapper.jar CUSTOM PARAMETERS ################
# Divisional ID - ask Mikel about "fourDigitId", meaning it was declared already but not defined in here.
#fourDigitId=2208
fourDigitId=XXXX
#The division date to run
sourceDate=XXXX
#Do we need this parameters prefixed with "${nameNode}?  I would say no.
#The source path of the the divisional data.
sourcePath=/user/financialDataFeed/data${fourDigitId}/${sourceName}/finished/${sourceDate}
#The destination where the data will land to be sqooped.
outputPath=/user/${sourceName}/financialdatafeed/data/finished
#The name of the entity to run, "" runs all entities.
entity=XXXX
#Mapping file contain valid practice IDs to run.
practiceMap=/enterprise/mappings/${sourceName}/chs-practice-id-mapping-athena.csv
#Mapping file containing valid entities to run.
entityMap=/enterprise/mappings/${sourceName}/athena_table_defs.csv
################# END JavaMapper.jar CUSTOM PARAMETERS ################

################## END CUSTOM ################
stageOneDataPartition=financialdatafeed
stageOneRawDir=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/raw
stageOneFinishedDir=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/finished
stageOneExtractedDir=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/extracted
stageOneErrorDir=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/error
stageOneOozieDir=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/oozie
workingFile=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/raw/WORKING.TXT
controlFile=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/raw/CONTROL.TXT
failureFile=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/raw/_FAILURE
jobFile=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/oozie/workflow.xml
oozie.coord.application.path=${nameNode}/user/${sourceName}/data/${stageOneDataPartition}/oozie/coordinator.xml
########
OOZIE
oozie.wf.rerun.failnodes=true
oozie.use.system.libpath=true
oozie.libpath=${nameNode}/enterprise/lib/athena
scriptsDir=${nameNode}/enterprise/scripts/athena
# Location of global sqoop-env.sh and sqoop-site.xml
# These are copied from /etc/sqoop/conf found on a sqoop-gateway node
sqoopConfDir=${nameNode}/enterprise/sqoopers/athena
# Email: TODO
toEmailAddress=hadoop_events@chs.net
###############
# COORDINATOR
coordStart=2015-02-26T00:00Z
coordEnd=2025-02-26T00:00Z
coordTimeZone=America/Chicago
coordTimeOut=20
coordConcurrency=1
coordExecution=FIFO


